{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import get_cmap\n",
    "from matplotlib.patches import Patch\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from category_encoders import *\n",
    "from IPython.display import display\n",
    "import scipy.stats as sp\n",
    "import datetime as dt\n",
    "import re\n",
    "#import categorical_embedder\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, RobustScaler, StandardScaler\n",
    "le = LabelEncoder()\n",
    "sns.set(rc={'figure.figsize':(14, 8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datainput = pd.read_csv(r\"C:\\Users\\Ighdaro Emwinghare\\Downloads\\filtered_req_calls.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datainput.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since we are interested in analyizing different product_ids, it is necessary to drop rows that contain null as product_id\n",
    "data = datainput\n",
    "data  = data.dropna(axis=0, subset=['product_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict(data['product_id'].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['monthyear'] = pd.to_datetime(datainput['date']).dt.to_period('M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill missing values with specific indicator.\n",
    "data['content_category'] = data['content_category'].fillna('Unknown')\n",
    "data['touchpoint_channel_clm'] = data['touchpoint_channel_clm'].fillna('Unknown')\n",
    "data['content_message_local'] = data['content_message_local'].fillna('NoMessage')\n",
    "data['content_message_global'] = data['content_message_global'].fillna('NoMessage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(data['content_message_local'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(data['content_message_global'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reform the content_message_concat by combining content_message_Local, 'sgm' and content_message_global\n",
    "data['tactic'] = data['content_message_local'] + ' SGM ' + data['content_message_global']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tactic'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tactic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()/len(data)\n",
    "\n",
    "#99% values in tactic, content_category, content_message_local and content_message_global are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Helper Functions\"\"\"\n",
    "\n",
    "\n",
    "def get_cat_feats(data=None):\n",
    "    '''\n",
    "    Returns the categorical features in a data set\n",
    "    Parameters:\n",
    "    -----------\n",
    "        data: DataFrame or named Series \n",
    "    Returns:\n",
    "    -------\n",
    "        List\n",
    "            A list of all the categorical features in a dataset.\n",
    "    it is used as a helper function for most of the functions to get categorical variables\n",
    "    '''\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame or Series, got 'None'\")\n",
    "\n",
    "    cat_features = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "    return list(cat_features)\n",
    "def get_num_feats(data=None):\n",
    "    '''\n",
    "    Returns the numerical features in a data set\n",
    "    Parameters:\n",
    "    -----------\n",
    "        data: DataFrame or named Series \n",
    "    Returns:\n",
    "    -------\n",
    "        List:\n",
    "            A list of all the numerical features in a dataset.\n",
    "    it is used as a helper function for most of the functions to get categorical variables\n",
    "    '''\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame or Series, got 'None'\")\n",
    "\n",
    "    num_features = data.select_dtypes(include=numerics).columns\n",
    "\n",
    "    return list(num_features)\n",
    "def get_unique_counts(data=None):\n",
    "    '''\n",
    "    Gets the unique count of categorical features in a data set.\n",
    "    Parameters\n",
    "    -----------\n",
    "        data: DataFrame or named Series \n",
    "    Returns\n",
    "    -------\n",
    "        DataFrame or Series\n",
    "            Unique value counts of the features in a dataset.\n",
    "    it is used as a helper function in the describe function to get the count of unique values in the columns \n",
    "    '''\n",
    "\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame or Series, got 'None'\")\n",
    "\n",
    "    features = get_cat_feats(data)\n",
    "    temp_len = []\n",
    "\n",
    "    for feature in features:\n",
    "        temp_len.append(len(data[feature].unique()))\n",
    "        \n",
    "    df = list(zip(features, temp_len))\n",
    "    df = pd.DataFrame(df, columns=['Feature', 'Unique Count'])\n",
    "    df = df.style.bar(subset=['Unique Count'], align='mid')\n",
    "    return df\n",
    "def display_missing(data=None, plot=False):\n",
    "    '''\n",
    "    Display missing values as a pandas dataframe.\n",
    "    Parameters\n",
    "    ----------\n",
    "        data: DataFrame or named Series\n",
    "        plot: bool, Default False\n",
    "            Plots missing values in dataset as a heatmap\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        Matplotlib Figure:\n",
    "            Heatmap plot of missing values\n",
    "    '''\n",
    "\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame or Series, got 'None'\")\n",
    "\n",
    "    df = data.isna().sum()\n",
    "    df = df.reset_index()\n",
    "    df.columns = ['features', 'missing_counts']\n",
    "\n",
    "    missing_percent = round((df['missing_counts'] / data.shape[0]) * 100, 1)\n",
    "    df['missing_percent'] = missing_percent\n",
    "\n",
    "    if plot:\n",
    "        plot_missing(data)\n",
    "        return df\n",
    "    else:\n",
    "        return df\n",
    "    \n",
    "def cat_summarizer(data, x=None, y=None, hue=None, palette='Set1', verbose=True):\n",
    "    '''\n",
    "    Helper function that gives a quick summary of a given column of categorical data\n",
    "    Parameters:\n",
    "    ---------------------------\n",
    "        dataframe: pandas dataframe\n",
    "        x: str.\n",
    "            horizontal axis to plot the labels of categorical data, y would be the count.\n",
    "        y: str. \n",
    "            vertical axis to plot the labels of categorical data, x would be the count.\n",
    "        hue: str. i\n",
    "            if you want to compare it another variable (usually the target variable)\n",
    "        palette: array, list.\n",
    "            Colour of the plot\n",
    "    Returns:\n",
    "    ----------------------\n",
    "        Quick Stats of the data and also the count plot\n",
    "        \n",
    "        it is used in the describe function\n",
    "    '''\n",
    "    if x == None:\n",
    "        column_interested = y\n",
    "    else:\n",
    "        column_interested = x\n",
    "    series = data[column_interested]\n",
    "    print(series.describe())\n",
    "    print('mode: ', series.mode())\n",
    "    if verbose:\n",
    "        print('='*80)\n",
    "        print(series.value_counts())\n",
    "\n",
    "    sns.countplot(x=x, y=y, hue=hue, data=data, palette=palette)\n",
    "    plt.show()\n",
    "    \n",
    "def _space():\n",
    "    '''it is used in  most functions to add space. this makes result more presentation'''\n",
    "    print('\\n')\n",
    "def _match_date(data):\n",
    "    '''\n",
    "        Return a list of columns that matches the DateTime expression\n",
    "    '''\n",
    "    mask = data.sample(20).astype(str).apply(lambda x : x.str.match(r'(\\d{2,4}-\\d{2}-\\d{2,4})+').all())\n",
    "    return set(data.loc[:, mask].columns)\n",
    "\n",
    "\n",
    "def display_rows(data,num=2):\n",
    "    '''\n",
    "    Displays the required number of rows\n",
    "    it is used in the describe function\n",
    "    '''\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame/ numpy2d array, got 'None'\")\n",
    "\n",
    "    return data.head(num)\n",
    "\n",
    "def plot_missing(data=None):\n",
    "    '''\n",
    "    Plots the data as a heatmap to show missing values\n",
    "    Parameters\n",
    "    ----------\n",
    "        data: DataFrame, array, or list of arrays.\n",
    "            The data to plot.\n",
    "    '''\n",
    "\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame or Series, got 'None'\")\n",
    "\n",
    "    sns.heatmap(data.isnull(), cbar=True)\n",
    "    plt.show()\n",
    "    \n",
    "def class_count(data=None, features=None, plot=False, save_fig=False):\n",
    "    '''\n",
    "    Displays the number of classes in a categorical feature.\n",
    "    Parameters:\n",
    "    \n",
    "        data: Pandas DataFrame or Series\n",
    "            Dataset for plotting.\n",
    "        features: Scalar, array, or list. \n",
    "            The categorical features in the dataset, if None, \n",
    "            we try to infer the categorical columns from the dataframe.\n",
    "        plot: bool, Default False.\n",
    "            Plots the class counts as a barplot\n",
    "        save_fig: bool, Default False.\n",
    "            Saves the plot to the current working directory.\n",
    "    it is used in the describe function\n",
    "    '''\n",
    "\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame or Series, got 'None'\")\n",
    "\n",
    "    if features is None:\n",
    "        features = get_cat_feats(data)\n",
    "\n",
    "                        \n",
    "\n",
    "    for feature in features:\n",
    "        if data[feature].nunique() > 15:\n",
    "            print(\"Unique classes in {} too large\".format(feature))\n",
    "        else:\n",
    "            print('Class Count for', feature)\n",
    "            display(pd.DataFrame(data[feature].value_counts()))\n",
    "\n",
    "    if plot:\n",
    "        countplot(data, features, save_fig=save_fig)\n",
    "        \n",
    "def get_date_cols(data=None):\n",
    "    '''\n",
    "    Returns the Datetime columns in a data set.\n",
    "    Parameters\n",
    "    ----------\n",
    "        data: DataFrame or named Series\n",
    "            Data set to infer datetime columns from.\n",
    "        convert: bool, Default True\n",
    "            Converts the inferred date columns to pandas DateTime type\n",
    "    Returns:\n",
    "    -------\n",
    "        List\n",
    "         Date column names in the data set\n",
    "    use in the describe function to set date columns to datetime datatype in utc\n",
    "    '''\n",
    "\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame or Series, got 'None'\")\n",
    "\n",
    "    #Get existing date columns in pandas Datetime64 format\n",
    "    date_cols = set(data.dtypes[data.dtypes == 'datetime64[ns, UTC]'].index)\n",
    "    #infer Date columns \n",
    "    date_cols = date_cols.union(_match_date(data))\n",
    "       \n",
    "    return date_cols\n",
    "\n",
    "def bivariate_stats(data):\n",
    "    '''Returns the contingency table and chi2 contingency test result between columns in the dataframe\n",
    "        \n",
    "        it is used in the describe function for categorical features analysis \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    cat_feats = get_cat_feats(data=data)\n",
    "    counter = 1\n",
    "    try:\n",
    "        while counter<(len(cat_feats)):\n",
    "            val1 = get_cat_feats(data=datainput)[counter - 1]\n",
    "            val2 = get_cat_feats(data=datainput)[counter]\n",
    "            if (data[val1].nunique() > 15) or (data[val2].nunique() > 15):\n",
    "                print('Number of unique values too large')\n",
    "            else:\n",
    "                freqtab = pd.crosstab(data[val1], data[val2])\n",
    "                print(\"Frequency table\")\n",
    "                print(\"============================\")\n",
    "                print(freqtab)\n",
    "                print(\"============================\")\n",
    "                chi2, pval, dof, expected = sp.chi2_contingency(freqtab)\n",
    "                print(\"ChiSquare test statistic: \",chi2)\n",
    "                print(\"p-value: \",pval)\n",
    "                _space()\n",
    "            counter= counter+1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "def bivariate_stats_target(data, target):\n",
    "    \n",
    "    '''Returns the contingency table and chi2 contingency test result between columns and the target variable in the dataframe\n",
    "        \n",
    "        \n",
    "        Parameters\n",
    "    ----------\n",
    "        data: DataFrame or named Series\n",
    "            Data set to infer datetime columns from.\n",
    "        target: the target variable in form of string\n",
    "\n",
    "        \n",
    "        it is used in the describe function for categorical features analysis of the relationship between the target variable \n",
    "        and other categorical features\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    cat_feats = get_cat_feats(data=data)\n",
    "    for i in cat_feats:\n",
    "        if (data[i].nunique() > 20):\n",
    "            print('Number of Unique values too large')\n",
    "        else:\n",
    "            freqtab = pd.crosstab(data[i], data[target])\n",
    "            print(\"Frequency table\")\n",
    "            print(\"============================\")\n",
    "            print(freqtab)\n",
    "            print(\"============================\")\n",
    "            chi2, pval, dof, expected = sp.chi2_contingency(freqtab)\n",
    "            print(\"ChiSquare test statistic: \",chi2)\n",
    "            print(\"p-value: \",pval)\n",
    "            _space()\n",
    "            \n",
    "def describe(data=None, name='', date_cols=None, show_categories=False, plot_missing=False, target = None):\n",
    "    '''\n",
    "    Calculates statistics and information about a data set. Information displayed are\n",
    "    shapes, size, number of categorical/numeric/date features, missing values,\n",
    "    dtypes of objects, correlation analysis, contigency analysis etc.\n",
    "    Parameters:\n",
    "    --------------------\n",
    "        data: Pandas DataFrame\n",
    "            The data to describe.\n",
    "        name: str, optional\n",
    "            The name of the data set passed to the function.\n",
    "        date_cols: list/series/array\n",
    "            Date column names in the data set.\n",
    "        show_categories: bool, default False\n",
    "            Displays the unique classes and counts in each of the categorical feature in the data set.\n",
    "        plot_missing: bool, default True\n",
    "            Plots missing values as a heatmap\n",
    "        target: the target variable in the dataframe\n",
    "    Returns:\n",
    "    -------\n",
    "        None\n",
    "        \n",
    "        This function is stand alone use for quick statistical exploration of the data.\n",
    "    '''\n",
    "    \n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame or Series, got 'None'\")\n",
    "\n",
    "    ## Get categorical features\n",
    "    cat_features = get_cat_feats(data)\n",
    "    \n",
    "    #Get numerical features\n",
    "    num_features = get_num_feats(data)\n",
    "\n",
    "    print('First five data points')\n",
    "    display(data.head())\n",
    "    _space()\n",
    "\n",
    "    print('Random five data points')\n",
    "    display(data.sample(5))\n",
    "    _space()\n",
    "\n",
    "    print('Last five data points')\n",
    "    display(data.tail())\n",
    "    _space()\n",
    "\n",
    "    print('Shape of {} data set: {}'.format(name, data.shape))\n",
    "    _space()\n",
    "\n",
    "    print('Size of {} data set: {}'.format(name, data.size))\n",
    "    _space()\n",
    "\n",
    "    print('Data Types')\n",
    "    print(\"Note: All Non-numerical features are identified as objects in pandas\")\n",
    "    display(pd.DataFrame(data.dtypes, columns=['Data Type']))\n",
    "    _space()\n",
    "    \n",
    "    date_cols = get_date_cols(data)\n",
    "    if len(date_cols) is not 0:\n",
    "        print(\"Column(s) {} should be in Datetime format. Use the [to_date] function to convert to Pandas Datetime format\".format(date_cols))\n",
    "        _space()\n",
    "\n",
    "    print('Numerical Features in Data set')\n",
    "    print(num_features)\n",
    "    _space()\n",
    "\n",
    "    print('Categorical Features in Data set')\n",
    "    display(cat_features)\n",
    "    _space()\n",
    "\n",
    "    print('Statistical Description of Columns')\n",
    "    display(data.describe())\n",
    "    _space()\n",
    "    \n",
    "    print('Description of Categorical Features')\n",
    "    if cat_features != None:\n",
    "        display(data.describe(include=[np.object, pd.Categorical]).T)\n",
    "        _space()\n",
    "          \n",
    "    print('Unique class Count of Categorical features')\n",
    "    display(get_unique_counts(data))\n",
    "    _space()\n",
    "\n",
    "    if show_categories:     \n",
    "        print('Classes in Categorical Columns')\n",
    "        print(\"-\"*30)\n",
    "        class_count(data, cat_features)\n",
    "        _space()\n",
    "\n",
    "    print('Missing Values in Data')\n",
    "    display(display_missing(data))\n",
    "    _space()\n",
    "  \n",
    "    print('Pearson Correlation')\n",
    "    print(data.corr())\n",
    "    _space()\n",
    "    \n",
    "    print('Kendall Correlation')\n",
    "    print(data.corr(method='kendall'))\n",
    "    _space()\n",
    "    \n",
    "    print('Spearman Correlation')\n",
    "    print(data.corr(method='spearman'))\n",
    "    _space()\n",
    "    \n",
    "    print('Bivariant Stats between categorical features')\n",
    "    print(bivariate_stats(data))\n",
    "    _space()\n",
    "    if target is not None:\n",
    "        print('Bivariant Stats between cat feats and target variable')\n",
    "        print(bivariate_stats_target(data, target))\n",
    "        _space()\n",
    "    print('')\n",
    "    \n",
    "\n",
    "def drop_missing(data=None, percent=99):\n",
    "    '''\n",
    "    Drops missing columns with [percent] of missing data.\n",
    "    Parameters:\n",
    "    -------------------------\n",
    "        data: Pandas DataFrame or Series.\n",
    "        percent: float, Default 99\n",
    "            Percentage of missing values to be in a column before it is eligible for removal.\n",
    "    Returns:\n",
    "    ------------------\n",
    "        Pandas DataFrame or Series.\n",
    "    It can be used alone. It also used in deal_with_missing_value function.\n",
    "    \n",
    "    This function is used in the deal_with_missing_value function.\n",
    "    '''\n",
    "\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame/ numpy2d array, got 'None'\")\n",
    "    \n",
    "    missing_percent = (data.isna().sum() / data.shape[0]) * 100\n",
    "    cols_2_drop = missing_percent[missing_percent.values >= percent].index\n",
    "    print(\"Dropped {}\".format(list(cols_2_drop)))\n",
    "    #Drop missing values\n",
    "    df = data.drop(cols_2_drop, axis=1)\n",
    "    return df\n",
    "\n",
    "def fill_missing_cats(data=None, cat_features=None, missing_encoding=None, missing_col=False):\n",
    "    '''\n",
    "    Fill missing values using the mode of the categorical features.\n",
    "    Parameters:\n",
    "    ------------------------\n",
    "        data: DataFrame or name Series.\n",
    "            Data set to perform operation on.\n",
    "        cat_features: List, Series, Array.\n",
    "            categorical features to perform operation on. If not provided, we automatically infer the categoricals from the dataset.\n",
    "        missing_encoding: List, Series, Array.\n",
    "            Values used in place of missing. Popular formats are [-1, -999, -99, '', ' ']\n",
    "        missin_col: bool, Default True\n",
    "      Creates a new column to capture the missing values. 1 if missing and 0 otherwise. This can sometimes help a machine learning model.\n",
    "      \n",
    "      This function is used in the deal_with_missing_value function.\n",
    "    '''\n",
    "\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame/ numpy2d array, got 'None'\")\n",
    "\n",
    "    if cat_features is None:\n",
    "        cat_features = get_cat_feats(data)\n",
    "\n",
    "    df = data.copy()\n",
    "    #change all possible missing values to NaN\n",
    "    if missing_encoding is None:\n",
    "        missing_encoding = ['', ' ', -99, -999]\n",
    "\n",
    "    df.replace(missing_encoding, np.NaN, inplace=True)\n",
    "    \n",
    "    for feat in cat_features:\n",
    "        if missing_col:\n",
    "            df[feat + '_missing_value'] = (df[feat].isna()).astype('int64')\n",
    "        most_freq = df[feat].mode()[0]\n",
    "        df[feat] = df[feat].replace(np.NaN, most_freq)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def fill_missing_num(data=None, num_features=None, method='mean', missing_col=False):\n",
    "    '''\n",
    "    fill missing values in numerical columns with specified [method] value\n",
    "    Parameters:\n",
    "        ------------------------------\n",
    "        data: DataFrame or name Series.\n",
    "            The data set to fill\n",
    "        features: list.\n",
    "            List of columns to fill\n",
    "        method: str, Default 'mean'.\n",
    "            method to use in calculating fill value.\n",
    "        missing_col: bool, Default True\n",
    "          Creates a new column to capture the missing values. 1 if missing and 0 otherwise. This can sometimes help a machine learning model.\n",
    "          \n",
    "          This function is used in the deal_with_missing_value function.\n",
    "    '''\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame/ numpy2d array, got 'None'\")\n",
    "    \n",
    "    if num_features is None:\n",
    "        num_features = get_num_feats(data)\n",
    "        #get numerical features with missing values\n",
    "        temp_df = data[num_features].isna().sum()\n",
    "        features = list(temp_df[num_features][temp_df[num_features] > 0].index)\n",
    "        \n",
    "    df = data.copy()\n",
    "    for feat in features:\n",
    "        if missing_col:\n",
    "            df[feat + '_missing_value'] = (df[feat].isna()).astype('int64')\n",
    "        if method is 'mean':\n",
    "            mean = df[feat].mean()\n",
    "            df[feat].fillna(mean, inplace=True)\n",
    "        elif method is 'median':\n",
    "            median = df[feat].median()\n",
    "            df[feat].fillna(median, inplace=True)\n",
    "        elif method is 'mode':\n",
    "            mode = df[feat].mode()[0]\n",
    "            df[feat].fillna(mode, inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\"method: must specify a fill method, one of [mean, mode or median]'\")\n",
    "    return df\n",
    "\n",
    "def deal_with_missing_value(data, percent=70):\n",
    "    \"\"\"\n",
    "    this function automatically take care of missing values.\n",
    "        It fills the missing values in categorical variables with mode of the particular column\n",
    "        and fills the missing value numerical variables with mean of the particular column.\n",
    "        It automatically drops columns with more than 70% missing values except when set otherwise.\n",
    "        \n",
    "        This function is used in the feature_preprocessing function to deal with missing values. It can also be used alone.\n",
    "        \"\"\"\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame/ numpy2d array, got 'None'\")\n",
    "    df1 = drop_missing(data=data, percent=percent)\n",
    "    df2 = fill_missing_cats(data=df1)\n",
    "    df = fill_missing_num(data=df2)\n",
    "    return df\n",
    "\n",
    "def drop_redundant(data):\n",
    "    '''\n",
    "    Removes features with the same value in all cell. Drops feature If Nan is the second unique class as well.\n",
    "    Parameters:\n",
    "    -----------------------------\n",
    "        data: DataFrame or named series.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame or named series.\n",
    "    This function is used in the feature_processing function.\n",
    "    '''\n",
    "\n",
    "    if data is None:\n",
    "        raise ValueError(\"data: Expecting a DataFrame/ numpy2d array, got 'None'\")\n",
    "    \n",
    "    #get columns\n",
    "    cols_2_drop = _nan_in_class(data)\n",
    "    print(\"Dropped {}\".format(cols_2_drop))\n",
    "    df = data.drop(cols_2_drop, axis=1)\n",
    "    return df\n",
    "def _nan_in_class(data):\n",
    "    \"\"\"helper function for drop_redundant function\"\"\"\n",
    "    cols = []\n",
    "    for col in data.columns:\n",
    "        if len(data[col].unique()) == 1:\n",
    "            cols.append(col)\n",
    "\n",
    "        if len(data[col].unique()) == 2:\n",
    "            if np.nan in list(data[col].unique()):\n",
    "                cols.append(col)\n",
    "\n",
    "    return cols\n",
    "\n",
    "#Label Encoding for object to numeric conversion\n",
    "def binaryencoder(data):\n",
    "    \"\"\"To avoid the curse of dimensionality, this function only encodes categorical features with less than 7 unique values\n",
    "    \n",
    "        This function can be used alone. It is also used in the encode_data function.\n",
    "        \n",
    "        It is the default method for encoding categorical variables with unique value less than four in the encode_data function.\n",
    "        \n",
    "        explanation: It is similar to onehot encoding but gives lesser dimensions, making it a better option. \n",
    "                    It converts the unique entry into binary combination and then creates column using binary hashing.\n",
    "    \"\"\"\n",
    "    features = get_cat_feats(data=data)\n",
    "    cols = []\n",
    "    for feature in features:\n",
    "        if data[feature].nunique() < 7:\n",
    "            cols.append(feature)\n",
    "        \n",
    "    enc = BinaryEncoder(cols=cols).fit(data)\n",
    "    data = enc.transform(data)\n",
    "    return data\n",
    "\n",
    "def onehotencoder(data):\n",
    "    \"\"\"To avoid the curse of dimensionality, this function only encodes categorical features with less than 4 unique values\n",
    "    \n",
    "        This function can be used alone. It is also used in the encode_data function\n",
    "        \n",
    "    The onehotencoder is only used when the number of unique value is less than four to avoid the curse of dimensionality.\n",
    "    If encode_data method parameter is set to 'onehotencode' this is what is used in encoding categorical variables with \n",
    "    number of unique values less than 4.\n",
    "    \n",
    "    explanation: it is used mostly for nominal variables such that a binary combination of the unique values are set as new columns\n",
    "                    in the dataset.\n",
    "    \"\"\"\n",
    "    features = get_cat_feats(data=data)\n",
    "    cols = []\n",
    "    for feature in features:\n",
    "        if data[feature].nunique() < 4:\n",
    "            cols.append(feature)\n",
    "        \n",
    "    enc = OneHotEncoder(cols=cols).fit(data)\n",
    "    data = enc.transform(data)\n",
    "    return data\n",
    "\n",
    "def labelencoder(data):\n",
    "    \"\"\"\n",
    "    This function can be used alone. It is also used in the encode_data function.\n",
    "    \n",
    "    It is used for columns that has more than 3 unique values. Such columns are treated as ordinal variables. \n",
    "    \n",
    "    Explanation: Label encoders are ordinal encoders that encode unique values as continuous intergers.\n",
    "    \n",
    "    \"\"\"\n",
    "    features = get_cat_feats(data=data)\n",
    "    for feat in features:\n",
    "        data[feat] = le.fit_transform(data[feat].astype(str))\n",
    "    return data\n",
    "\n",
    "def sumencoder(data):\n",
    "    \"\"\"\n",
    "    This function can be used alone. It is also used in the encode_data function.\n",
    "    \n",
    "    The sumencoder is only used when the number of unique value is less than four to avoid the curse of dimensionality.\n",
    "    If encode_data method parameter is set to 'sumencode' this is what is used in encoding categorical variables with \n",
    "    number of unique values less than 4.\n",
    "    \n",
    "    explanation: it is similar to one-hot encoding but the difference is that in sum encoding one value is taken as '-1'\n",
    "                and it is not compared to other value.\n",
    "    \"\"\"\n",
    "\n",
    "    features = get_cat_feats(data=data)\n",
    "    cols = []\n",
    "    for feature in features:\n",
    "        if data[feature].nunique() < 4:\n",
    "            cols.append(feature)\n",
    "    enc = SumEncoder(cols = cols).fit_transform(data)\n",
    "    data = enc\n",
    "    return data\n",
    "\n",
    "def catboostencoder(data, target):\n",
    "    '''Data inputs must not be string\n",
    "        This function is used alone. It is not called by any other function.\n",
    "        \n",
    "        it uses the catboost tree model in properly encoding categorical features.\n",
    "    \n",
    "        explanation: a target encoder. It uses the target variable in encoding the categorical variables. \n",
    "        It is more accurate than most encoding methods.\n",
    "    '''\n",
    "    X = data.drop(target, axis=1)\n",
    "    y = data[target]\n",
    "    features = get_cat_feats(data=X)\n",
    "    enc = CatBoostEncoder(cols=features).fit(X,y)\n",
    "    data = enc.transform(X, y)\n",
    "    return data\n",
    "\n",
    "def hashencoder(data):\n",
    "    '''\n",
    "         This function can be used alone. It is also used in the encode_data function.\n",
    "    \n",
    "        The hashencoder is only used when the number of unique value is less than four to avoid the curse of dimensionality.\n",
    "        If encode_data method parameter is set to 'hashencode' this is what is used in encoding categorical variables with \n",
    "        number of unique values less than 4.\n",
    "\n",
    "        explanation: Feature hashing maps each category in a categorical feature to an integer within a predetermined range\n",
    "    \n",
    "                        The size of the output dimensions is controlled by the variable n_components.\n",
    "    '''    \n",
    "    \n",
    "    cols = get_cat_feats(data)\n",
    "    new_col = []\n",
    "    for i in cols:\n",
    "        string = str(data[i][0]) + str(data[i][len(data)-1]) \n",
    "        flag = re.findall(r'\\d+', string)\n",
    "        if len(flag) > 2:\n",
    "               if len(flag[0])>2:\n",
    "                    new_col.append(i)\n",
    "        \n",
    "    enc = HashingEncoder(cols=new_col, n_components= 1).fit(data)\n",
    "    data = enc.transform(data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def embeddingencoder(data):\n",
    "     \"\"\"\n",
    "        This function is used alone.\n",
    "        It uses neural network embeddings to encode categorical features. \n",
    "         \"\"\"\n",
    "     embedding_info = ce.get_embedding_info(data)\n",
    "     X_encoded,encoders = ce.get_label_encoded_data(data)\n",
    "\n",
    "     return X_endoded\n",
    "\n",
    "def encode_data(data, method='binary'):\n",
    "    \"\"\"\n",
    "        encodes categorical variables automatically using binary encoding for columns with less than 4 unique values\n",
    "        then label encode all other variables\n",
    "        method takes either binary or onehot or sumencode or hashcode. default is binary\n",
    "        \n",
    "        this function can be used alone and it also used in the feature_processing function.\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    if method == 'binary':\n",
    "        data = binaryencoder(data)\n",
    "    elif method== 'onehot':\n",
    "        data = onehotencoder(data)\n",
    "    elif method == 'sumencode':\n",
    "        data = sumencode(data)\n",
    "    data = labelencoder(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = deal_with_missing_value(data, percent=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = drop_redundant(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(data=data, name='', date_cols=None, show_categories=False, plot_missing=False, target = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['frequency_of_content_category'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped1 = data.groupby(by = ['product_id','monthyear', 'content_category'])['frequency_of_content_category'].sum().reset_index()\n",
    "\n",
    "# No 'content_category', 'content_message_local', 'content_message_global', 'product', 'indication', 'therapeutic_area', 'segment_quant', 'touchpoint_channel_clm','account_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped2 = data.groupby(by = ['product_id','monthyear', 'content_message_local',\n",
    "       'content_message_global', 'product', 'indication', 'therapeutic_area', 'segment_quant', 'touchpoint_channel_clm',\n",
    "       'tactic', 'account_id', 'content_category'])['frequency_of_content_category'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped3 = data.groupby(by = ['product_id','monthyear', 'tactic','content_message_local',\n",
    "       'content_message_global', 'product', 'indication', 'therapeutic_area', 'segment_quant', 'touchpoint_channel_clm',\n",
    "       'content_category'])['frequency_of_content_category'].sum().reset_index()\n",
    "\n",
    "# No 'account_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouped = data.groupby(by = ['product_id','content_category', 'content_message_local',\n",
    "#       'content_message_global', 'product', 'indication', 'therapeutic_area', 'segment_quant', 'touchpoint_channel_clm',\n",
    "#       'content_message_concat', 'monthyear'])['frequency_of_tactic'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped1['product_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped2['product_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped3['product_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data=grouped3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = dict(grouped_data['content_category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_diff_content_categories= values.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1\n",
    "mapper = dict()\n",
    "for x in list_of_diff_content_categories:\n",
    "    mapper.update( {x : counter} )\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " pd.DataFrame(mapper.items(), columns=['content_category', 'encoded_content_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data['encoded_content_category'] = grouped_data['content_category'].map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(data=grouped_data, name='', date_cols=None, show_categories=False, plot_missing='frequency_of_tactic', target = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data['month'] = grouped_data['monthyear'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data['year'] = grouped_data['monthyear'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data1 = grouped_data[grouped_data['product']=='Skyrizi_PS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data2 = grouped_data[grouped_data['product']=='Skyrizi_KAM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data1 = new_data1.drop(['product_id', 'monthyear'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data2 = new_data2.drop(['product_id', 'monthyear'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data1 = encode_data(new_data1, method='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data2 = encode_data(new_data2, method='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data1 = new_data1.drop('tactic', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_data2 = new_data2.drop('tactic', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Skyrizi_PS')\n",
    "sns.heatmap(new_data1.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Skyrizi_KAM')\n",
    "sns.heatmap(new_data2.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Skyrizi_PS')\n",
    "new_data1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Skyrizi_KAM')\n",
    "new_data2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardscaler(data, cols = None):\n",
    "    ''' \n",
    "    It is a standard normalization technique use when using machine learning models that assusme a normal/gaussian distribution.\n",
    "    Models like Linear Regression, Gaussian Naive Bayes etc.\n",
    "    -------------\n",
    "        data: DataFrame, named Series\n",
    "            Data set to perform operation on. It advisable not to scale/normalise the target variable.\n",
    "        col: list of str\n",
    "            columns in form of a list to scale/normalise. If not parsed, it scales/normalises the entire dataframe\n",
    "        Returns:\n",
    "    --------\n",
    "        DataFrame of the scaled/normalised data/columns.\n",
    "        \n",
    "    It is used in the scale_normalise_data. It can also be used alone.\n",
    "     '''\n",
    "    if cols is not None:\n",
    "        \n",
    "        s_scaler = StandardScaler()\n",
    "        data[cols] = s_scaler.fit_transform(data[cols])\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        col_names = data.columns\n",
    "        s_scaler = StandardScaler()\n",
    "        df_s = s_scaler.fit_transform(data)\n",
    "        data = pd.DataFrame(df_s, columns=col_names)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "y = new_data1['frequency_of_content_category'].reset_index()\n",
    "X = standardscaler(new_data1.drop('frequency_of_content_category', axis=1))\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Content Message Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data['product_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "df1 = grouped_data[['encoded_content_category','frequency_of_content_category','monthyear']]\n",
    "df1.index = df1['monthyear']\n",
    "pt = pd.pivot_table(df1, index=df1.index.month, columns=df1['encoded_content_category'], aggfunc='sum')['frequency_of_content_category']\n",
    "\n",
    "ax = plt.figure().add_subplot(111)\n",
    "ax.plot(pt)\n",
    "\n",
    "ticklabels = [datetime.date(1900, item, 1).strftime('%b') for item in pt.index]\n",
    "ax.set_xticks(np.arange(1,13))\n",
    "ax.set_xticklabels(ticklabels) #add monthlabels to the xaxis\n",
    "\n",
    "ax.legend(pt.columns.tolist(), loc='center left', bbox_to_anchor=(1, .5)) #add the column names as legend.\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1\n",
    "mapper = dict()\n",
    "for x in list_of_diff_content_categories:\n",
    "    mapper.update( {x : f'content_category_{counter}'} )\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data['encoded_content_category'] = grouped_data['content_category'].map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data['encoded_content_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data['Date'] = grouped_data['monthyear'].values.astype('datetime64[M]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data['product_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Message Concat Analysis for Product a00G000000URPpNIAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = grouped_data[grouped_data['product_id']=='a00G000000URPpNIAX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hue_order = list(dict(df1['encoded_content_category'].value_counts()).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['encoded_content_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(data=df1, x='Date', y='frequency_of_content_category', hue='encoded_content_category', ax=ax, estimator='sum',ci = None)\n",
    "#ax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"%Y-%M\"))\n",
    "fig.autofmt_xdate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.groupby(['monthyear', 'encoded_content_category'])['frequency_of_content_category'].sum().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Message Concat Analysis for Product a00G000000URPpGIAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = grouped_data[grouped_data['product_id']=='a00G000000URPpGIAX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(dict(df2['encoded_content_category'].value_counts()).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(data=df2, x='Date', y='frequency_of_content_category', hue='encoded_content_category', ax=ax, estimator='sum',ci = None)\n",
    "#ax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"%Y-%M\"))\n",
    "fig.autofmt_xdate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.groupby(['monthyear', 'encoded_content_category'])['frequency_of_content_category'].sum().unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Message Concat Analysis for Product a00G000000URPpMIAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = grouped_data[grouped_data['product_id']=='a00G000000URPpMIAX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(dict(df3['encoded_content_category'].value_counts()).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(data=df3, x='Date', y='frequency_of_content_category',ci = None, hue='encoded_content_category', ax=ax, estimator='sum')\n",
    "#ax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"%Y-%M\"))\n",
    "fig.autofmt_xdate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.groupby(['monthyear', 'encoded_content_category'])['frequency_of_content_category'].sum().unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Message Concat Analysis for Product a00G000000URPppIAH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = grouped_data[grouped_data['product_id']=='a00G000000URPppIAH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(dict(df4['encoded_content_category'].value_counts()).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(data=df4, x='Date', y='frequency_of_content_category',ci = None, hue='encoded_content_category', ax=ax, estimator='sum')\n",
    "#ax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"%Y-%M\"))\n",
    "fig.autofmt_xdate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.groupby(['monthyear', 'encoded_content_category'])['frequency_of_content_category'].sum().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Content Message Concat Analysis for Product a00G000000URPpkIAH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = grouped_data[grouped_data['product_id']=='a00G000000URPpkIAH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(dict(df5['encoded_content_category'].value_counts()).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(data=df5, x='Date', y='frequency_of_content_category',ci = None, hue='encoded_content_category', ax=ax, estimator='sum')\n",
    "#ax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"%Y-%M\"))\n",
    "fig.autofmt_xdate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.groupby(['monthyear', 'encoded_content_category'])['frequency_of_content_category'].sum().unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Content Message Concat Analysis for Product a001v00001wbtlnAAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = grouped_data[grouped_data['product_id']=='a001v00001wbtlnAAA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(dict(df6['encoded_content_category'].value_counts()).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(df6['encoded_content_category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_list = ['content_category_1',\n",
    " 'content_category_2',\n",
    " 'content_category_3',\n",
    " 'content_category_4',\n",
    " 'content_category_5']\n",
    "df_1 = df6[df6['encoded_content_category'].isin(value_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_list = ['content_category_6',\n",
    " 'content_category_7',\n",
    " 'content_category_10',\n",
    " 'content_category_11',\n",
    " 'content_category_12']\n",
    "df_2 = df6[df6['encoded_content_category'].isin(value_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [df_1, df_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in col:\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.lineplot(data=x, x='Date', y='frequency_of_content_category', hue='encoded_content_category', ax=ax, estimator='sum',ci = None)\n",
    "    #ax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"%Y-%M\"))\n",
    "    fig.autofmt_xdate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.groupby(['monthyear', 'encoded_content_category'])['frequency_of_content_category'].sum().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Content Message Concat Analysis for Product Skyrizi_KAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = grouped_data[grouped_data['product']=='Skyrizi_KAM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(dict(df7['encoded_content_category'].value_counts()).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(data=df7, x='Date', y='frequency_of_content_category',ci = None, hue='encoded_content_category', ax=ax, estimator='sum')\n",
    "#ax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"%Y-%M\"))\n",
    "fig.autofmt_xdate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7.groupby(['monthyear', 'encoded_content_category'])['frequency_of_content_category'].sum().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Content Message Concat Analysis for Product Skyrizi_PS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8 = grouped_data[grouped_data['product']=='Skyrizi_PS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(dict(df8['encoded_content_category'].value_counts()).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(df8['encoded_content_category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_list = ['content_category_1',\n",
    " 'content_category_2',\n",
    " 'content_category_3',\n",
    " 'content_category_4',\n",
    " 'content_category_5']\n",
    "df_1 = df8[df8['encoded_content_category'].isin(value_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_list = ['content_category_6',\n",
    " 'content_category_7',\n",
    " 'content_category_10',\n",
    " 'content_category_11',\n",
    " 'content_category_12']\n",
    "df_2 = df8[df8['encoded_content_category'].isin(value_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [df_1, df_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in col:\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.lineplot(data=x, x='Date', y='frequency_of_content_category', hue='encoded_content_category', ax=ax, estimator='sum',ci = None)\n",
    "    #ax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"%Y-%M\"))\n",
    "    fig.autofmt_xdate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tactic and Actual Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(mapper.items(), columns=['content_category', 'encoded_content_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
